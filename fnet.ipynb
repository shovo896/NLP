{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93e42f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.fft as fft\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "311da84b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9d04d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "datasets=load_dataset('wikitext','wikitext-2-raw-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6bd98ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:4: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:4: SyntaxWarning: invalid escape sequence '\\s'\n",
      "/var/folders/79/qf4s258d1979pqm6v_z63bmc0000gn/T/ipykernel_1583/1055278940.py:4: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  text=re.sub('\\s\\s',' ',text)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f47f28c1b7944e1d996c0ca82a8fe6c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/19530 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f4ca45c3b4d4493a284d4e73f1d5602",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2083 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dceb9b056f44cfcbd097aac6e05aa54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2383 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dc0a5208674469fa2a7a3ae3a406d99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/19530 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a39ddbad5954aea8e06a68f3ef94d67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/2083 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fabec10f4e6e44b3bcb3ce8e56e89b62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/2383 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess_function(sentence):\n",
    "    text=sentence['text'].lower()\n",
    "    text=re.sub(r'[^a-zA-Z0-9\\s]','',text)\n",
    "    text=re.sub('\\s\\s',' ',text)\n",
    "    sentence['text']=text\n",
    "    return sentence\n",
    "datasets['train']=datasets['train'].map(preprocess_function)\n",
    "datasets['validation']=datasets['validation'].map(preprocess_function)\n",
    "datasets['test']=datasets['test'].map(preprocess_function)  \n",
    "\n",
    "\n",
    "datasets['train']=datasets['train'].filter(lambda example: len(example['text'])>20)\n",
    "datasets['validation']=datasets['validation'].filter(lambda example: len(example['text'])>20)\n",
    "datasets['test']=datasets['test'].filter(lambda example: len(example['text'])>20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cd4034",
   "metadata": {},
   "source": [
    "tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8ff2d23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d9615c3f2894766a0911e8a60f41751",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1265cf9ab2cd43c89844f19d1e8bbf20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a671ac3ab4f41248add675ef028d67f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c885ae5df7d4e36a7801af872fdcb62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2333 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f14f345fde5c49438791dd1a0db35403",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/19067 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a43b4215b954349886b99541daa9865",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2034 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorWithPadding \n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint='distilbert-base-uncased-finetuned-sst-2-english'\n",
    "tokenizer=AutoTokenizer.from_pretrained(checkpoint)\n",
    "def tokenize(sentence):\n",
    "    sentence=tokenizer(sentence['text'],truncation=True,max_length=512)\n",
    "    return sentence\n",
    "tokenized_inputs=datasets.map(tokenize,batched=True)\n",
    "tokenized_inputs.remove_columns(['text'])\n",
    "batch=16\n",
    "data_collator=DataCollatorWithPadding(tokenizer=tokenizer,padding=True,return_tensors='pt')\n",
    "dataloader=DataLoader(tokenized_inputs['train'],batch_size=batch,shuffle=True,collate_fn=data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7146aed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_sequence_length):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        pe = self._generate_positional_encoding()\n",
    "        # Register as buffer so it moves with .to(device)\n",
    "        self.register_buffer(\"positional_encoding\", pe)\n",
    "\n",
    "    def _generate_positional_encoding(self):\n",
    "        positional_encoding = np.zeros((self.max_sequence_length, self.d_model))\n",
    "        for pos in range(self.max_sequence_length):\n",
    "            for i in range(0, self.d_model, 2):\n",
    "                positional_encoding[pos, i] = np.sin(pos / (10000 ** ((2 * i) / self.d_model)))\n",
    "                if i + 1 < self.d_model:\n",
    "                    positional_encoding[pos, i + 1] = np.cos(pos / (10000 ** ((2 * (i + 1)) / self.d_model)))\n",
    "        return torch.from_numpy(positional_encoding).float()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len, d_model)\n",
    "        pe = self.positional_encoding.unsqueeze(0).expand(x.size(0), -1, -1)\n",
    "        return x + pe[:, : x.size(1), :]\n",
    "\n",
    "\n",
    "class PositionalEncodding(nn.Module):\n",
    "    def __init__(self, sequence_length, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.position_embedding = PositionalEncoding(embed_dim, sequence_length)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded_tokens = self.token_embedding(x)\n",
    "        return self.position_embedding(embedded_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4548361d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FNetEncoder(nn.Module):\n",
    "    def __init__(self,embed_dim,ffn_dim,dropout):\n",
    "        super(FNetEncoder,self).__init__()\n",
    "        self.layer_norm1=nn.LayerNorm(embed_dim)\n",
    "        self.layer_norm2=nn.LayerNorm(embed_dim)\n",
    "        self.ffn=nn.Sequential(\n",
    "            nn.Linear(embed_dim,ffn_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(ffn_dim,embed_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        y=self.layer_norm1(x)\n",
    "        y_fft=fft.fft2(y).real\n",
    "        x=x+y_fft\n",
    "        y=self.layer_norm2(x)\n",
    "        y=self.ffn(y)\n",
    "        output=x+y\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f8633839",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FNetDecoder(nn.Module):\n",
    "    def __init__(self,num_layers,sequence_length,vocab_size,embed_dim,ffn_dim,dropout):\n",
    "        super(FNetDecoder,self).__init__()\n",
    "        self.embedding=PositionalEncodding(sequence_length,vocab_size,embed_dim)\n",
    "        self.layers=nn.ModuleList([FNetEncoder(embed_dim,ffn_dim,dropout) for _ in range(num_layers)])\n",
    "        self.layer_norm=nn.LayerNorm(embed_dim)\n",
    "        self.output_layer=nn.Linear(embed_dim,vocab_size)\n",
    "    def forward(self,x):\n",
    "        x=self.embedding(x)\n",
    "        for layer in self.layers:\n",
    "            x=layer(x)\n",
    "        x=self.layer_norm(x)\n",
    "        output=self.output_layer(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d839a670",
   "metadata": {},
   "source": [
    "FNet model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f549be0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FNetModel(nn.Module):\n",
    "    def __init__(self,num_layers,sequence_length,vocab_size,embed_dim,ffn_dim,dropout):\n",
    "        super(FNetModel,self).__init__()\n",
    "        self.decoder=FNetDecoder(num_layers,sequence_length,vocab_size,embed_dim,ffn_dim,dropout)\n",
    "    def forward(self,x):\n",
    "        output=self.decoder(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "94448fe9",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "PositionalEncodding.__init__() takes 3 positional arguments but 4 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m optimizer=torch.optim.AdamW\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m model=\u001b[43mFNetModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m6\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m               \u001b[49m\u001b[43msequence_length\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m512\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m               \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m               \u001b[49m\u001b[43membed_dim\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m256\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m               \u001b[49m\u001b[43mffn_dim\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m               \u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m.to(device)\n\u001b[32m      8\u001b[39m loss_fn=nn.CrossEntropyLoss()\n\u001b[32m      9\u001b[39m optimizer=optimizer(model.parameters(), lr=\u001b[32m1e-4\u001b[39m)    \n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36mFNetModel.__init__\u001b[39m\u001b[34m(self, num_layers, sequence_length, vocab_size, embed_dim, ffn_dim, dropout)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m,num_layers,sequence_length,vocab_size,embed_dim,ffn_dim,dropout):\n\u001b[32m      3\u001b[39m     \u001b[38;5;28msuper\u001b[39m(FNetModel,\u001b[38;5;28mself\u001b[39m).\u001b[34m__init__\u001b[39m()\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     \u001b[38;5;28mself\u001b[39m.decoder=\u001b[43mFNetDecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43msequence_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43mffn_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36mFNetDecoder.__init__\u001b[39m\u001b[34m(self, num_layers, sequence_length, vocab_size, embed_dim, ffn_dim, dropout)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m,num_layers,sequence_length,vocab_size,embed_dim,ffn_dim,dropout):\n\u001b[32m      3\u001b[39m     \u001b[38;5;28msuper\u001b[39m(FNetDecoder,\u001b[38;5;28mself\u001b[39m).\u001b[34m__init__\u001b[39m()\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     \u001b[38;5;28mself\u001b[39m.embedding=\u001b[43mPositionalEncodding\u001b[49m\u001b[43m(\u001b[49m\u001b[43msequence_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43membed_dim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m     \u001b[38;5;28mself\u001b[39m.layers=nn.ModuleList([FNetEncoder(embed_dim,ffn_dim,dropout) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_layers)])\n\u001b[32m      6\u001b[39m     \u001b[38;5;28mself\u001b[39m.layer_norm=nn.LayerNorm(embed_dim)\n",
      "\u001b[31mTypeError\u001b[39m: PositionalEncodding.__init__() takes 3 positional arguments but 4 were given"
     ]
    }
   ],
   "source": [
    "optimizer=torch.optim.AdamW\n",
    "model=FNetModel(num_layers=6,\n",
    "               sequence_length=512,\n",
    "               vocab_size=tokenizer.vocab_size,\n",
    "               embed_dim=256,\n",
    "               ffn_dim=1024,\n",
    "               dropout=0.1).to(device)\n",
    "loss_fn=nn.CrossEntropyLoss()\n",
    "optimizer=optimizer(model.parameters(), lr=1e-4)    \n",
    "epochs=10\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss=0\n",
    "    for batch in dataloader:\n",
    "        inputs=batch['input_ids'].to(device)\n",
    "        attention_mask=batch['attention_mask'].to(device)\n",
    "        labels=inputs.clone().to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs=model(inputs)\n",
    "        loss=loss_fn(outputs.view(-1,tokenizer.vocab_size),labels.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss+=loss.item()\n",
    "    avg_loss=total_loss/len(dataloader)\n",
    "    print(f'Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
