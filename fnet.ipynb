{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93e42f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.fft as fft\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "311da84b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9d04d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "datasets=load_dataset('wikitext','wikitext-2-raw-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6bd98ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:4: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:4: SyntaxWarning: invalid escape sequence '\\s'\n",
      "/var/folders/79/qf4s258d1979pqm6v_z63bmc0000gn/T/ipykernel_1583/1055278940.py:4: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  text=re.sub('\\s\\s',' ',text)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f47f28c1b7944e1d996c0ca82a8fe6c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/19530 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f4ca45c3b4d4493a284d4e73f1d5602",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2083 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dceb9b056f44cfcbd097aac6e05aa54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2383 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dc0a5208674469fa2a7a3ae3a406d99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/19530 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a39ddbad5954aea8e06a68f3ef94d67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/2083 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fabec10f4e6e44b3bcb3ce8e56e89b62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/2383 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess_function(sentence):\n",
    "    text=sentence['text'].lower()\n",
    "    text=re.sub(r'[^a-zA-Z0-9\\s]','',text)\n",
    "    text=re.sub('\\s\\s',' ',text)\n",
    "    sentence['text']=text\n",
    "    return sentence\n",
    "datasets['train']=datasets['train'].map(preprocess_function)\n",
    "datasets['validation']=datasets['validation'].map(preprocess_function)\n",
    "datasets['test']=datasets['test'].map(preprocess_function)  \n",
    "\n",
    "\n",
    "datasets['train']=datasets['train'].filter(lambda example: len(example['text'])>20)\n",
    "datasets['validation']=datasets['validation'].filter(lambda example: len(example['text'])>20)\n",
    "datasets['test']=datasets['test'].filter(lambda example: len(example['text'])>20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cd4034",
   "metadata": {},
   "source": [
    "tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8ff2d23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d9615c3f2894766a0911e8a60f41751",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1265cf9ab2cd43c89844f19d1e8bbf20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a671ac3ab4f41248add675ef028d67f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c885ae5df7d4e36a7801af872fdcb62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2333 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f14f345fde5c49438791dd1a0db35403",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/19067 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a43b4215b954349886b99541daa9865",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2034 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorWithPadding \n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint='distilbert-base-uncased-finetuned-sst-2-english'\n",
    "tokenizer=AutoTokenizer.from_pretrained(checkpoint)\n",
    "def tokenize(sentence):\n",
    "    sentence=tokenizer(sentence['text'],truncation=True,max_length=512)\n",
    "    return sentence\n",
    "tokenized_inputs=datasets.map(tokenize,batched=True)\n",
    "tokenized_inputs.remove_columns(['text'])\n",
    "batch=16\n",
    "data_collator=DataCollatorWithPadding(tokenizer=tokenizer,padding=True,return_tensors='pt')\n",
    "dataloader=DataLoader(tokenized_inputs['train'],batch_size=batch,shuffle=True,collate_fn=data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7146aed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_sequence_length):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        pe = self._generate_positional_encoding()\n",
    "        # Register as buffer so it moves with .to(device)\n",
    "        self.register_buffer(\"positional_encoding\", pe)\n",
    "\n",
    "    def _generate_positional_encoding(self):\n",
    "        positional_encoding = np.zeros((self.max_sequence_length, self.d_model))\n",
    "        for pos in range(self.max_sequence_length):\n",
    "            for i in range(0, self.d_model, 2):\n",
    "                positional_encoding[pos, i] = np.sin(pos / (10000 ** ((2 * i) / self.d_model)))\n",
    "                if i + 1 < self.d_model:\n",
    "                    positional_encoding[pos, i + 1] = np.cos(pos / (10000 ** ((2 * (i + 1)) / self.d_model)))\n",
    "        return torch.from_numpy(positional_encoding).float()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len, d_model)\n",
    "        pe = self.positional_encoding.unsqueeze(0).expand(x.size(0), -1, -1)\n",
    "        return x + pe[:, : x.size(1), :]\n",
    "\n",
    "\n",
    "class PositionalEncodding(nn.Module):\n",
    "    def __init__(self, sequence_length, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.position_embedding = PositionalEncoding(embed_dim, sequence_length)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded_tokens = self.token_embedding(x)\n",
    "        return self.position_embedding(embedded_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4548361d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FNetEncoder(nn.Module):\n",
    "    def __init__(self,embed_dim,ffn_dim,dropout):\n",
    "        super(FNetEncoder,self).__init__()\n",
    "        self.layer_norm1=nn.LayerNorm(embed_dim)\n",
    "        self.layer_norm2=nn.LayerNorm(embed_dim)\n",
    "        self.ffn=nn.Sequential(\n",
    "            nn.Linear(embed_dim,ffn_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(ffn_dim,embed_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        y=self.layer_norm1(x)\n",
    "        y_fft=fft.fft2(y).real\n",
    "        x=x+y_fft\n",
    "        y=self.layer_norm2(x)\n",
    "        y=self.ffn(y)\n",
    "        output=x+y\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f8633839",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FNetDecoder(nn.Module):\n",
    "    def __init__(self,num_layers,sequence_length,vocab_size,embed_dim,ffn_dim,dropout):\n",
    "        super(FNetDecoder,self).__init__()\n",
    "        self.embedding=PositionalEncodding(sequence_length,vocab_size,embed_dim)\n",
    "        self.layers=nn.ModuleList([FNetEncoder(embed_dim,ffn_dim,dropout) for _ in range(num_layers)])\n",
    "        self.layer_norm=nn.LayerNorm(embed_dim)\n",
    "        self.output_layer=nn.Linear(embed_dim,vocab_size)\n",
    "    def forward(self,x):\n",
    "        x=self.embedding(x)\n",
    "        for layer in self.layers:\n",
    "            x=layer(x)\n",
    "        x=self.layer_norm(x)\n",
    "        output=self.output_layer(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d839a670",
   "metadata": {},
   "source": [
    "FNet model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f549be0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FNetModel(nn.Module):\n",
    "    def __init__(self,num_layers,sequence_length,vocab_size,embed_dim,ffn_dim,dropout):\n",
    "        super(FNetModel,self).__init__()\n",
    "        self.decoder=FNetDecoder(num_layers,sequence_length,vocab_size,embed_dim,ffn_dim,dropout)\n",
    "    def forward(self,x):\n",
    "        output=self.decoder(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "94448fe9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`text` in this case) have excessive nesting (inputs type `list` where type `int` is expected).",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:731\u001b[39m, in \u001b[36mBatchEncoding.convert_to_tensors\u001b[39m\u001b[34m(self, tensor_type, prepend_batch_axis)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor(value):\n\u001b[32m--> \u001b[39m\u001b[32m731\u001b[39m     tensor = \u001b[43mas_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    733\u001b[39m     \u001b[38;5;66;03m# Removing this for now in favor of controlling the shape with `prepend_batch_axis`\u001b[39;00m\n\u001b[32m    734\u001b[39m     \u001b[38;5;66;03m# # at-least2d\u001b[39;00m\n\u001b[32m    735\u001b[39m     \u001b[38;5;66;03m# if tensor.ndim > 2:\u001b[39;00m\n\u001b[32m    736\u001b[39m     \u001b[38;5;66;03m#     tensor = tensor.squeeze(0)\u001b[39;00m\n\u001b[32m    737\u001b[39m     \u001b[38;5;66;03m# elif tensor.ndim < 2:\u001b[39;00m\n\u001b[32m    738\u001b[39m     \u001b[38;5;66;03m#     tensor = tensor[None, :]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:690\u001b[39m, in \u001b[36mBatchEncoding.convert_to_tensors.<locals>.as_tensor\u001b[39m\u001b[34m(value, dtype)\u001b[39m\n\u001b[32m    689\u001b[39m     dtype = torch.int64\n\u001b[32m--> \u001b[39m\u001b[32m690\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mValueError\u001b[39m: too many dimensions 'str'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     12\u001b[39m model.train()\n\u001b[32m     13\u001b[39m total_loss=\u001b[32m0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43minput_ids\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mattention_mask\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/data/dataloader.py:631\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    628\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    629\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    630\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m631\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    632\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    633\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[32m    634\u001b[39m         \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[32m    635\u001b[39m         \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/data/dataloader.py:675\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    673\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    674\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m675\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    676\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    677\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     52\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     53\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/data/data_collator.py:225\u001b[39m, in \u001b[36mDataCollatorWithPadding.__call__\u001b[39m\u001b[34m(self, features)\u001b[39m\n\u001b[32m    224\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, features: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]) -> \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[32m--> \u001b[39m\u001b[32m225\u001b[39m     batch = \u001b[43mpad_without_fast_tokenizer_warning\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    226\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    227\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    228\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    229\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    230\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    231\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    232\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    233\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m batch:\n\u001b[32m    234\u001b[39m         batch[\u001b[33m\"\u001b[39m\u001b[33mlabels\u001b[39m\u001b[33m\"\u001b[39m] = batch[\u001b[33m\"\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/data/data_collator.py:63\u001b[39m, in \u001b[36mpad_without_fast_tokenizer_warning\u001b[39m\u001b[34m(tokenizer, *pad_args, **pad_kwargs)\u001b[39m\n\u001b[32m     60\u001b[39m tokenizer.deprecation_warnings[\u001b[33m\"\u001b[39m\u001b[33mAsking-to-pad-a-fast-tokenizer\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m     padded = \u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpad_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpad_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     65\u001b[39m     \u001b[38;5;66;03m# Restore the state of the warning.\u001b[39;00m\n\u001b[32m     66\u001b[39m     tokenizer.deprecation_warnings[\u001b[33m\"\u001b[39m\u001b[33mAsking-to-pad-a-fast-tokenizer\u001b[39m\u001b[33m\"\u001b[39m] = warning_state\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2784\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.pad\u001b[39m\u001b[34m(self, encoded_inputs, padding, max_length, pad_to_multiple_of, padding_side, return_attention_mask, return_tensors, verbose)\u001b[39m\n\u001b[32m   2781\u001b[39m             batch_outputs[key] = []\n\u001b[32m   2782\u001b[39m         batch_outputs[key].append(value)\n\u001b[32m-> \u001b[39m\u001b[32m2784\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mBatchEncoding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:240\u001b[39m, in \u001b[36mBatchEncoding.__init__\u001b[39m\u001b[34m(self, data, encoding, tensor_type, prepend_batch_axis, n_sequences)\u001b[39m\n\u001b[32m    236\u001b[39m     n_sequences = encoding[\u001b[32m0\u001b[39m].n_sequences\n\u001b[32m    238\u001b[39m \u001b[38;5;28mself\u001b[39m._n_sequences = n_sequences\n\u001b[32m--> \u001b[39m\u001b[32m240\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconvert_to_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtensor_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprepend_batch_axis\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepend_batch_axis\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:747\u001b[39m, in \u001b[36mBatchEncoding.convert_to_tensors\u001b[39m\u001b[34m(self, tensor_type, prepend_batch_axis)\u001b[39m\n\u001b[32m    742\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m key == \u001b[33m\"\u001b[39m\u001b[33moverflowing_tokens\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    743\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    744\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mUnable to create tensor returning overflowing tokens of different lengths. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    745\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mPlease see if a fast version of this tokenizer is available to have this feature available.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    746\u001b[39m             ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m747\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    748\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mUnable to create tensor, you should probably activate truncation and/or padding with\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    749\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m\u001b[33mpadding=True\u001b[39m\u001b[33m'\u001b[39m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m\u001b[33mtruncation=True\u001b[39m\u001b[33m'\u001b[39m\u001b[33m to have batched tensors with the same length. Perhaps your\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    750\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m features (`\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m` in this case) have excessive nesting (inputs type `list` where type `int` is\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    751\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m expected).\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    752\u001b[39m         ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    754\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[31mValueError\u001b[39m: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`text` in this case) have excessive nesting (inputs type `list` where type `int` is expected)."
     ]
    }
   ],
   "source": [
    "optimizer=torch.optim.AdamW\n",
    "model=FNetModel(num_layers=6,\n",
    "               sequence_length=512,\n",
    "               vocab_size=tokenizer.vocab_size,\n",
    "               embed_dim=256,\n",
    "               ffn_dim=1024,\n",
    "               dropout=0.1).to(device)\n",
    "loss_fn=nn.CrossEntropyLoss()\n",
    "optimizer=optimizer(model.parameters(), lr=1e-4)    \n",
    "epochs=10\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss=0\n",
    "    for batch in dataloader:\n",
    "        inputs=batch['input_ids'].to(device)\n",
    "        attention_mask=batch['attention_mask'].to(device)\n",
    "        labels=inputs.clone().to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs=model(inputs)\n",
    "        loss=loss_fn(outputs.view(-1,tokenizer.vocab_size),labels.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss+=loss.item()\n",
    "    avg_loss=total_loss/len(dataloader)\n",
    "    print(f'Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
