{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93e42f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.fft as fft\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "311da84b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9d04d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "datasets=load_dataset('wikitext','wikitext-2-raw-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6bd98ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:4: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:4: SyntaxWarning: invalid escape sequence '\\s'\n",
      "/var/folders/79/qf4s258d1979pqm6v_z63bmc0000gn/T/ipykernel_5486/1055278940.py:4: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  text=re.sub('\\s\\s',' ',text)\n"
     ]
    }
   ],
   "source": [
    "def preprocess_function(sentence):\n",
    "    text=sentence['text'].lower()\n",
    "    text=re.sub(r'[^a-zA-Z0-9\\s]','',text)\n",
    "    text=re.sub('\\s\\s',' ',text)\n",
    "    sentence['text']=text\n",
    "    return sentence\n",
    "datasets['train']=datasets['train'].map(preprocess_function)\n",
    "datasets['validation']=datasets['validation'].map(preprocess_function)\n",
    "datasets['test']=datasets['test'].map(preprocess_function)  \n",
    "\n",
    "\n",
    "datasets['train']=datasets['train'].filter(lambda example: len(example['text'])>20)\n",
    "datasets['validation']=datasets['validation'].filter(lambda example: len(example['text'])>20)\n",
    "datasets['test']=datasets['test'].filter(lambda example: len(example['text'])>20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cd4034",
   "metadata": {},
   "source": [
    "tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e8ff2d23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b03ff617d4f4efc95ecbde5506dec9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2333 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "920fab4cb43842588531bfeb1a71987c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/19067 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92ed3302e20044e5ad2c74c38e87bb2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2034 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorWithPadding \n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint='distilbert-base-uncased-finetuned-sst-2-english'\n",
    "tokenizer=AutoTokenizer.from_pretrained(checkpoint)\n",
    "def tokenize(sentence):\n",
    "    return tokenizer(sentence['text'],truncation=True,max_length=512)\n",
    "tokenized_inputs=datasets.map(tokenize,batched=True)\n",
    "tokenized_inputs=tokenized_inputs.remove_columns(['text'])\n",
    "batch=16\n",
    "data_collator=DataCollatorWithPadding(tokenizer=tokenizer,padding=True,return_tensors='pt')\n",
    "dataloader=DataLoader(tokenized_inputs['train'],batch_size=batch,shuffle=True,collate_fn=data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7146aed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_sequence_length):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        pe = self._generate_positional_encoding()\n",
    "        # Register as buffer so it moves with .to(device)\n",
    "        self.register_buffer(\"positional_encoding\", pe)\n",
    "\n",
    "    def _generate_positional_encoding(self):\n",
    "        positional_encoding = np.zeros((self.max_sequence_length, self.d_model))\n",
    "        for pos in range(self.max_sequence_length):\n",
    "            for i in range(0, self.d_model, 2):\n",
    "                positional_encoding[pos, i] = np.sin(pos / (10000 ** ((2 * i) / self.d_model)))\n",
    "                if i + 1 < self.d_model:\n",
    "                    positional_encoding[pos, i + 1] = np.cos(pos / (10000 ** ((2 * (i + 1)) / self.d_model)))\n",
    "        return torch.from_numpy(positional_encoding).float()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len, d_model)\n",
    "        pe = self.positional_encoding.unsqueeze(0).expand(x.size(0), -1, -1)\n",
    "        return x + pe[:, : x.size(1), :]\n",
    "\n",
    "\n",
    "class PositionalEncodding(nn.Module):\n",
    "    def __init__(self, sequence_length, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.position_embedding = PositionalEncoding(embed_dim, sequence_length)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded_tokens = self.token_embedding(x)\n",
    "        return self.position_embedding(embedded_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4548361d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FNetEncoder(nn.Module):\n",
    "    def __init__(self,embed_dim,ffn_dim,dropout):\n",
    "        super(FNetEncoder,self).__init__()\n",
    "        self.layer_norm1=nn.LayerNorm(embed_dim)\n",
    "        self.layer_norm2=nn.LayerNorm(embed_dim)\n",
    "        self.ffn=nn.Sequential(\n",
    "            nn.Linear(embed_dim,ffn_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(ffn_dim,embed_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        y=self.layer_norm1(x)\n",
    "        y_fft=fft.fft2(y).real\n",
    "        x=x+y_fft\n",
    "        y=self.layer_norm2(x)\n",
    "        y=self.ffn(y)\n",
    "        output=x+y\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f8633839",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FNetDecoder(nn.Module):\n",
    "    def __init__(self,num_layers,sequence_length,vocab_size,embed_dim,ffn_dim,dropout):\n",
    "        super(FNetDecoder,self).__init__()\n",
    "        self.embedding=PositionalEncodding(sequence_length,vocab_size,embed_dim)\n",
    "        self.layers=nn.ModuleList([FNetEncoder(embed_dim,ffn_dim,dropout) for _ in range(num_layers)])\n",
    "        self.layer_norm=nn.LayerNorm(embed_dim)\n",
    "        self.output_layer=nn.Linear(embed_dim,vocab_size)\n",
    "    def forward(self,x):\n",
    "        x=self.embedding(x)\n",
    "        for layer in self.layers:\n",
    "            x=layer(x)\n",
    "        x=self.layer_norm(x)\n",
    "        output=self.output_layer(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d839a670",
   "metadata": {},
   "source": [
    "FNet model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f549be0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FNetModel(nn.Module):\n",
    "    def __init__(self,num_layers,sequence_length,vocab_size,embed_dim,ffn_dim,dropout):\n",
    "        super(FNetModel,self).__init__()\n",
    "        self.decoder=FNetDecoder(num_layers,sequence_length,vocab_size,embed_dim,ffn_dim,dropout)\n",
    "    def forward(self,x):\n",
    "        output=self.decoder(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "94448fe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 4.0001\n",
      "Epoch 2/10, Loss: 1.4923\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     16\u001b[39m labels=inputs.clone().to(device)\n\u001b[32m     17\u001b[39m optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m outputs=\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m loss=loss_fn(outputs.view(-\u001b[32m1\u001b[39m,tokenizer.vocab_size),labels.view(-\u001b[32m1\u001b[39m))\n\u001b[32m     20\u001b[39m loss.backward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1510\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1511\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1515\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1516\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1517\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1518\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1519\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1520\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1522\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1523\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36mFNetModel.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,x):\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     output=\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1510\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1511\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1515\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1516\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1517\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1518\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1519\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1520\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1522\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1523\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mFNetDecoder.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m      9\u001b[39m x=\u001b[38;5;28mself\u001b[39m.embedding(x)\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.layers:\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     x=\u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m x=\u001b[38;5;28mself\u001b[39m.layer_norm(x)\n\u001b[32m     13\u001b[39m output=\u001b[38;5;28mself\u001b[39m.output_layer(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1510\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1511\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1515\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1516\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1517\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1518\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1519\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1520\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1522\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1523\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mFNetEncoder.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,x):\n\u001b[32m     14\u001b[39m     y=\u001b[38;5;28mself\u001b[39m.layer_norm1(x)\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m     y_fft=\u001b[43mfft\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfft2\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m.real\n\u001b[32m     16\u001b[39m     x=x+y_fft\n\u001b[32m     17\u001b[39m     y=\u001b[38;5;28mself\u001b[39m.layer_norm2(x)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "model=FNetModel(num_layers=6,\n",
    "               sequence_length=512,\n",
    "               vocab_size=tokenizer.vocab_size,\n",
    "               embed_dim=256,\n",
    "               ffn_dim=1024,\n",
    "               dropout=0.1).to(device)\n",
    "loss_fn=nn.CrossEntropyLoss()\n",
    "optimizer=torch.optim.AdamW(model.parameters(), lr=1e-4)    \n",
    "epochs=10\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss=0\n",
    "    for batch in dataloader:\n",
    "        inputs=batch['input_ids'].to(device)\n",
    "        attention_mask=batch['attention_mask'].to(device)\n",
    "        labels=inputs.clone().to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs=model(inputs)\n",
    "        loss=loss_fn(outputs.view(-1,tokenizer.vocab_size),labels.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss+=loss.item()\n",
    "    avg_loss=total_loss/len(dataloader)\n",
    "    print(f'Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6bbe2231",
   "metadata": {},
   "outputs": [],
   "source": [
    "## do yoursel f evaluation here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
